{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c95d4783",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dafccba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from utils.sgd import *\n",
    "\n",
    "from utils.utils import normalizeRows, softmax, sigmoid\n",
    "from utils.treebank import StanfordSentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cedd8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naiveSoftmaxLossAndGradient(\n",
    "    centerWordVec,\n",
    "    outsideWordIdx,\n",
    "    outsideVectors,\n",
    "    dataset\n",
    "):\n",
    "    \"\"\" Naive Softmax loss & gradient function for word2vec models\n",
    "\n",
    "    Implementation of  the naive softmax loss and gradients between a center word's \n",
    "    embedding and an outside word's embedding. This will be the building block\n",
    "    for our word2vec models. \n",
    "\n",
    "    Arguments:\n",
    "    centerWordVec -- numpy ndarray, center word's embedding\n",
    "                    in shape (word vector length, )\n",
    "                    (v_c in the pdf handout)\n",
    "    outsideWordIdx -- integer, the index of the outside word\n",
    "                    (o of u_o in the pdf handout)\n",
    "    outsideVectors -- outside vectors is\n",
    "                    in shape (num words in vocab, word vector length) \n",
    "                    for all words in vocab (tranpose of U in the pdf handout)\n",
    "    dataset -- needed for negative sampling, unused here.\n",
    "\n",
    "    Return:\n",
    "    loss -- naive softmax loss\n",
    "    gradCenterVec -- the gradient with respect to the center word vector\n",
    "                     in shape (word vector length, )\n",
    "                     (dJ / dv_c in the pdf handout)\n",
    "    gradOutsideVecs -- the gradient with respect to all the outside word vectors\n",
    "                    in shape (num words in vocab, word vector length) \n",
    "                    (dJ / dU)\n",
    "    \"\"\"\n",
    "\n",
    "    softmax_vect = softmax(outsideVectors @ centerWordVec)\n",
    "    loss = -np.log(softmax_vect[outsideWordIdx])\n",
    "\n",
    "    gradCenterVec = - outsideVectors[outsideWordIdx] + outsideVectors.T @ softmax_vect\n",
    "\n",
    "    gradOutsideVecs = softmax_vect.reshape(-1, 1) @ centerWordVec.reshape(1, -1)\n",
    "    gradOutsideVecs[outsideWordIdx] = (softmax_vect[outsideWordIdx] - 1) * centerWordVec # w != o\n",
    "    \n",
    "\n",
    "    \n",
    "    return loss, gradCenterVec, gradOutsideVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ea6558",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd45388c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28d4d7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNegativeSamples(outsideWordIdx, dataset, K):\n",
    "    \"\"\" Samples K indexes which are not the outsideWordIdx \"\"\"\n",
    "\n",
    "    negSampleWordIndices = [None] * K\n",
    "    for k in range(K):\n",
    "        newidx = dataset.sampleTokenIdx()\n",
    "        while newidx == outsideWordIdx:\n",
    "            newidx = dataset.sampleTokenIdx()\n",
    "        negSampleWordIndices[k] = newidx\n",
    "    return negSampleWordIndices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6239877",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "359ca37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def negSamplingLossAndGradient(\n",
    "    centerWordVec,\n",
    "    outsideWordIdx,\n",
    "    outsideVectors,\n",
    "    dataset,\n",
    "    K=10\n",
    "):\n",
    "    \"\"\" Negative sampling loss function for word2vec models\n",
    "\n",
    "    Implementation of  the negative sampling loss and gradients for \n",
    "    a centerWordVec and a outsideWordIdx word vector as a building block\n",
    "    for word2vec models. K is the number of negative samples to take.\n",
    "\n",
    "    Note: The same word may be negatively sampled multiple times. \n",
    "\n",
    "    Arguments/Return Specifications: same as naiveSoftmaxLossAndGradient\n",
    "    \"\"\"\n",
    "\n",
    "    negSampleWordIndices = getNegativeSamples(outsideWordIdx, dataset, K)\n",
    "    indices = [outsideWordIdx] + negSampleWordIndices\n",
    "\n",
    "    scores = outsideVectors[indices] @ centerWordVec # (K+1,)\n",
    "    sigmoid_neg_score_ks = sigmoid(-scores[1:]).reshape(-1, 1) # (K, 1)\n",
    "\n",
    "    loss = -np.log(sigmoid(scores[0])) - np.sum(np.log(sigmoid_neg_score_ks))\n",
    "\n",
    "    tmp_k = (1 - sigmoid_neg_score_ks) * outsideVectors[negSampleWordIndices] # (K, d)\n",
    "    gradCenterVec = (sigmoid(scores[0]) - 1) * outsideVectors[outsideWordIdx] \\\n",
    "        + np.sum(tmp_k, axis=0) # (d,)\n",
    "\n",
    "    gradOutsideVecs = np.zeros_like(outsideVectors) # (V, d)\n",
    "    gradOutsideVecs[outsideWordIdx] = (sigmoid(scores[0]) - 1) * centerWordVec # (d,)\n",
    "    # Count for duplications:\n",
    "    for i, k in enumerate(negSampleWordIndices):\n",
    "        gradOutsideVecs[k] += (1 - sigmoid_neg_score_ks[i]) * centerWordVec # (d, )\n",
    "\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return loss, gradCenterVec, gradOutsideVecs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4431cff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "def239b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def skipgram(currentCenterWord, windowSize, outsideWords, word2Ind,\n",
    "             centerWordVectors, outsideVectors, dataset,\n",
    "             word2vecLossAndGradient=naiveSoftmaxLossAndGradient):\n",
    "    \"\"\" Skip-gram model in word2vec\n",
    "\n",
    "    Implement the skip-gram model in this function.\n",
    "\n",
    "    Arguments:\n",
    "    currentCenterWord -- a string of the current center word\n",
    "    windowSize -- integer, context window size\n",
    "    outsideWords -- list of no more than 2*windowSize strings, the outside words\n",
    "    word2Ind -- a dictionary that maps words to their indices in\n",
    "              the word vector list\n",
    "    centerWordVectors -- center word vectors (as rows) is in shape \n",
    "                        (num words in vocab, word vector length) \n",
    "                        for all words in vocab (V in pdf handout)\n",
    "    outsideVectors -- outside vectors is in shape \n",
    "                        (num words in vocab, word vector length) \n",
    "                        for all words in vocab (transpose of U in the pdf handout)\n",
    "    word2vecLossAndGradient -- the loss and gradient function for\n",
    "                               a prediction vector given the outsideWordIdx\n",
    "                               word vectors, could be one of the two\n",
    "                               loss functions you implemented above.\n",
    "\n",
    "    Return:\n",
    "    loss -- the loss function value for the skip-gram model\n",
    "            (J in the pdf handout)\n",
    "    gradCenterVec -- the gradient with respect to the center word vector\n",
    "                     in shape (num words in vocab, word vector length)\n",
    "                     (dJ / dv_c in the pdf handout)\n",
    "    gradOutsideVecs -- the gradient with respect to all the outside word vectors\n",
    "                    in shape (num words in vocab, word vector length) \n",
    "                    (dJ / dU)\n",
    "    \"\"\"\n",
    "\n",
    "    loss = 0.0\n",
    "    gradCenterVecs = np.zeros(centerWordVectors.shape)\n",
    "    gradOutsideVectors = np.zeros(outsideVectors.shape)\n",
    "\n",
    "    ### YOUR CODE HERE (~8 Lines)\n",
    "\n",
    "    centerWordVec = centerWordVectors[word2Ind[currentCenterWord]]\n",
    "        \n",
    "    for outsideWord in outsideWords:\n",
    "        loss_j, gradCenterVecs_j, gradOutsideVectors_j = word2vecLossAndGradient(\n",
    "            centerWordVec, word2Ind[outsideWord], outsideVectors, dataset)\n",
    "        \n",
    "        loss += loss_j\n",
    "        gradCenterVecs[word2Ind[currentCenterWord]] += gradCenterVecs_j\n",
    "        gradOutsideVectors += gradOutsideVectors_j\n",
    "\n",
    "\n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return loss, gradCenterVecs, gradOutsideVectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f312320",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9c3b02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d71243",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec_sgd_wrapper(word2vecModel, word2Ind, wordVectors, dataset,\n",
    "                         windowSize,\n",
    "                         word2vecLossAndGradient=naiveSoftmaxLossAndGradient):\n",
    "    batchsize = 50\n",
    "    loss = 0.0\n",
    "    grad = np.zeros(wordVectors.shape)\n",
    "    N = wordVectors.shape[0]\n",
    "    centerWordVectors = wordVectors[:int(N/2),:]\n",
    "    outsideVectors = wordVectors[int(N/2):,:]\n",
    "    for i in range(batchsize):\n",
    "        windowSize1 = random.randint(1, windowSize)\n",
    "        centerWord, context = dataset.getRandomContext(windowSize1)\n",
    "\n",
    "        c, gin, gout = word2vecModel(\n",
    "            centerWord, windowSize1, context, word2Ind, centerWordVectors,\n",
    "            outsideVectors, dataset, word2vecLossAndGradient\n",
    "        )\n",
    "        loss += c / batchsize\n",
    "        grad[:int(N/2), :] += gin / batchsize\n",
    "        grad[int(N/2):, :] += gout / batchsize\n",
    "\n",
    "    return loss, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79873d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c810d144",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(314)\n",
    "dataset = StanfordSentiment()\n",
    "tokens = dataset.tokens()\n",
    "nWords = len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b518a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to train 10-dimensional vectors for this assignment\n",
    "dimVectors = 10\n",
    "\n",
    "# Context size\n",
    "C = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7212377a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3111cfed",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sgd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12836\\4129371849.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m        dimVectors, np.zeros((nWords, dimVectors))),\n\u001b[0;32m      5\u001b[0m     axis=0)\n\u001b[1;32m----> 6\u001b[1;33m wordVectors = sgd(\n\u001b[0m\u001b[0;32m      7\u001b[0m     lambda vec: word2vec_sgd_wrapper(skipgram, tokens, vec, dataset, C,\n\u001b[0;32m      8\u001b[0m         negSamplingLossAndGradient),\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sgd' is not defined"
     ]
    }
   ],
   "source": [
    "startTime=time.time()\n",
    "wordVectors = np.concatenate(\n",
    "    ((np.random.rand(nWords, dimVectors) - 0.5) /\n",
    "       dimVectors, np.zeros((nWords, dimVectors))),\n",
    "    axis=0)\n",
    "wordVectors = sgd(\n",
    "    lambda vec: word2vec_sgd_wrapper(skipgram, tokens, vec, dataset, C,\n",
    "        negSamplingLossAndGradient),\n",
    "    wordVectors, 0.3, 40000, None, True, PRINT_EVERY=10)\n",
    "# Note that normalization is not called here. This is not a bug,\n",
    "# normalizing during training loses the notion of length.\n",
    "\n",
    "print(\"sanity check: cost at convergence should be around or below 10\")\n",
    "print(\"training took %d seconds\" % (time.time() - startTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f6c4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordVectors = np.concatenate(\n",
    "    (wordVectors[:nWords,:], wordVectors[nWords:,:]),\n",
    "    axis=0)\n",
    "\n",
    "visualizeWords = [\n",
    "    \"great\", \"cool\", \"brilliant\", \"wonderful\", \"well\", \"amazing\",\n",
    "    \"worth\", \"sweet\", \"enjoyable\", \"boring\", \"bad\", \"dumb\",\n",
    "    \"annoying\", \"female\", \"male\", \"queen\", \"king\", \"man\", \"woman\", \"rain\", \"snow\",\n",
    "    \"hail\", \"coffee\", \"tea\"]\n",
    "\n",
    "visualizeIdx = [tokens[word] for word in visualizeWords]\n",
    "visualizeVecs = wordVectors[visualizeIdx, :]\n",
    "temp = (visualizeVecs - np.mean(visualizeVecs, axis=0))\n",
    "covariance = 1.0 / len(visualizeIdx) * temp.T.dot(temp)\n",
    "U,S,V = np.linalg.svd(covariance)\n",
    "coord = temp.dot(U[:,0:2])\n",
    "\n",
    "for i in range(len(visualizeWords)):\n",
    "    plt.text(coord[i,0], coord[i,1], visualizeWords[i],\n",
    "        bbox=dict(facecolor='green', alpha=0.1))\n",
    "\n",
    "plt.xlim((np.min(coord[:,0]), np.max(coord[:,0])))\n",
    "plt.ylim((np.min(coord[:,1]), np.max(coord[:,1])))\n",
    "\n",
    "plt.savefig('word_vectors.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs224n",
   "language": "python",
   "name": "cs224n"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
